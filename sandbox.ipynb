{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from yolo import Config, PostProcess, create_converter, create_model, draw_bboxes\n",
    "from yolo.utils.model_utils import get_device\n",
    "\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(project_root)\n",
    "\n",
    "CONFIG_PATH = \"../YOLO/yolo/config\"\n",
    "CONFIG_NAME = \"config\"\n",
    "CLASS_NUM = 80\n",
    "\n",
    "image_size = (640, 640)\n",
    "\n",
    "with initialize(config_path=CONFIG_PATH, version_base=None, job_name=\"notebook_job\"):\n",
    "    cfg: Config = compose(config_name=CONFIG_NAME)\n",
    "\n",
    "device, _ = get_device(cfg.device)\n",
    "model = create_model(\n",
    "    cfg.model,\n",
    "    class_num=CLASS_NUM,\n",
    "    # weight_path=\"/Users/simone.bonato/Desktop/ecolution/IFC_DL/submodules/YOLO/weights/v9-c.pt\",\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = get_config_for_notebook(\n",
    "    \"/Users/simone.bonato/Desktop/ecolution/IFC_DL/src/ifc_dl/conf\"\n",
    ")\n",
    "\n",
    "augs = {\n",
    "    \"resize\": {\n",
    "        \"params\": {\"height\": image_size[0], \"width\": image_size[1], \"interpolation\": 3},\n",
    "        \"all_datasets\": True,\n",
    "    },\n",
    "    \"horizontal_flip\": {\"params\": {\"p\": 0.5}},\n",
    "}\n",
    "\n",
    "transforms, val_transform = get_transform_fn(augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MockCOCODataset(partition=\"train\", transforms=transforms)\n",
    "datamodule = MockCocoDataModule(transforms=transforms, val_transform=val_transform)\n",
    "\n",
    "x, y = dataset[1]\n",
    "\n",
    "plot_instance_segmentation_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(x[None])\n",
    "\n",
    "model_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "train_dl = datamodule.train_dataloader()\n",
    "val_dl = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "\n",
    "x = torch.stack(x)\n",
    "\n",
    "model_output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = converter(model_output[\"Main\"])\n",
    "\n",
    "c[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but why is the shape like this? (it was [10, 8400, 80] when I wrote this, and the images had a shape of 640x640)\n",
    "\n",
    "because for every possible anchor box we get a prediction\n",
    "in the config for the anchor boxes we had strides of [8, 16, 32], if you divide the original image in cells of these shapes, you will see that the total is 8400\n",
    "\n",
    "Do: $$(640 / 8) ^2 + (640 / 16)^2 + (640 / 32)^2 = 8400$$ (anchors with stride 8) + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the shapes of the output\n",
    "\n",
    "There are 3 tuples, one for each resolution (60x80, 30x40, 15x20).\n",
    "Within each tuple we have predictions for: class, objects and bounding boxes.\n",
    "\n",
    "Takin the first tuple for example:\n",
    "\n",
    "**Class predictions**:\n",
    "- 80 is the number of object classes\n",
    "- 60x80 is the feature map size \n",
    "\n",
    "**Object predictions**:\n",
    "\t•\t1: Batch size\n",
    "\t•\t16: reg_max = 16 → number of bins\n",
    "\t•\t4: 4 box coordinates: [left, top, right, bottom]\n",
    "\t•\t80, 80: Grid size (spatial locations)\n",
    "\n",
    "**Bounding box predictions**:\n",
    "- 4 channels for bounding boxes\n",
    "- 60x80 is the feature map size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Main output shapes:\")\n",
    "for o in model_output[\"Main\"]:\n",
    "    for el in o:\n",
    "        print(el.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"\\nAUX output shapes:\")\n",
    "for o in model_output[\"AUX\"]:\n",
    "    for el in o:\n",
    "        print(el.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed = post_proccess(model_output)\n",
    "\n",
    "for pred in post_processed:\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bboxes(x, post_processed, idx2label=cfg.dataset.class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the loss function, the **DUAL LOSS**\n",
    "\n",
    "It is made of the following losses:\n",
    "- BCELoss (Binary Cross-Entropy Loss)\n",
    "- DFLoss (Distribution Focal Loss)\n",
    "- BoxLoss\n",
    "\n",
    "#### How should the loss inputs look like?\n",
    "\n",
    "- `aux_predicts`: the post-processed outputs of the model, for the \"Aux\" key. The model output some vector, and the converter is used to turn them into normal boxes.\n",
    "- `main_predicts`: same as `aux_predicts` but for the \"Main\" key.\n",
    "- `targets`: The ground truth class and bounding box information as tensor of size `[batch x n_targets x 5]`. But since here we must consider all the images in the batch, they might have different numbers of elements. For this reason the \"non-elements\" must be labeled with `-1` to keep the shape consistent. Hence `n_targets` should be the max_number of elements in the batch (will have to make a `custom collate_fn` for this in the dataloader)\n",
    "\n",
    "```\n",
    "aux_predicts = converter(model_output[\"AUX\"])\n",
    "main_predicts = converter(model_output[\"Main\"])\n",
    "\n",
    "loss_val = loss(\n",
    "    aux_predicts,\n",
    "    main_predicts,\n",
    "    targets=yolo_style_loss,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo.tools.loss_functions import create_loss_function\n",
    "\n",
    "loss = create_loss_function(cfg, converter)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scale_idx = 0\n",
    "\n",
    "class_vec = torch.zeros(6, 1)\n",
    "batch_size = model_output[\"Main\"][pred_scale_idx][0].shape[0]\n",
    "yolo_style_loss = torch.cat([class_vec, y[\"boxes\"]], dim=1)[None].repeat(\n",
    "    batch_size, 1, 1\n",
    ")\n",
    "\n",
    "aux_predicts = converter(model_output[\"AUX\"])\n",
    "main_predicts = converter(model_output[\"Main\"])\n",
    "\n",
    "loss_val = loss(\n",
    "    aux_predicts,\n",
    "    main_predicts,\n",
    "    targets=yolo_style_loss,\n",
    ")\n",
    "\n",
    "loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataloader\n",
    "\n",
    "Our dataloader should return:\n",
    "- the `images`, as always normalised and with shape `BATCH_SIZE, 3, H, W`\n",
    "- the `batch_targets`, which is a tensor of shape `BATCH_SIZE, max_annotations_in_batch_image, 5`. The 5 represents: `class object` (-1 for filler values), and `4 bbox coordinates` (probably `x_min, y_min, x_max, y_max`). Hence `[class, x_min, y_min, x_max, y_max]`\n",
    "\n",
    "\n",
    "Extra:\n",
    "- `rev_tensor` contains a tensor that can be used in the augmentation class they have to reverse the augmentation. We probably do not need this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo.tools.data_loader import create_dataloader\n",
    "\n",
    "dl = create_dataloader(cfg.task.data, cfg.dataset, cfg.task.task)\n",
    "batch = next(iter(dl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
